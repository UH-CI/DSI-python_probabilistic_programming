{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hRX6YD6myAkV"
   },
   "source": [
    "# Bayesian Inference\n",
    "\n",
    "---\n",
    "<img src = \"media/chapter3/bayesiansteps_intro-01.jpg\" width=\"300\" align=\"right\">\n",
    "\n",
    "The principles behind Bayesian inference are applied in probabilistic programming. Bayesian inference is just a mathematical way of predicting an outcome using probabilistic intuition. We can use previous observations, or data, to predict future outcomes.\n",
    "\n",
    "Let's recap what we've covered so far to get a better idea of how this all relates to Bayesian inference. In Chapter 1, we introduced the concepts of generative models and applying Bayesian inference. Recall, that we stated that the Bayesian approach can be summarized in five general steps (shown in the figure on the right).  \n",
    "\n",
    "In Chapter 2, we introduced basics of probability and discussed five different commonly used probability distributions: the binomial, Poisson, discrete uniform, continuous uniform, and Gaussian distributions. These probability distributions are relevant to Bayesian inference because both previous (or prior) and future (or posterior) knowledge are represented with probability distributions.\n",
    "\n",
    "Now, in Chapter 3, we are going to show you how to actually use and understand Bayesian inference and apply these concepts to probabilistic programming in PyMC3.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oetBcQFqXC6o"
   },
   "source": [
    "## Conditional Probability\n",
    "\n",
    "---\n",
    "\n",
    "The <a id='conditional'>*conditional probability*</a> refers to the likelihood that an event will occur given the occurrence of a different event. We can refer to these events as Event $A$ and Event $B$, where Event $B$ has already occurred and we are interested in the probability that Event $A$ will occur in addition to Event $B$. The conditional probability of Event $A$ occurring given the occurrence of Event $B$ is denoted as: $P(A|B)$. We use Bayes Theorem to compute conditional probability.\n",
    "\n",
    "## Bayes Theorem\n",
    "---\n",
    "<a id='Bayes'>*Bayes Theorem*</a> is a way to calculate the conditional probability:\n",
    "\n",
    "\n",
    "<center><img src = \"media/chapter3/bayestheorem.png\" width = 750></center>\n",
    "\n",
    "\n",
    "It states that the conditional probability of Event $A$ occurring given the occurrence of Event $B$ is equal to the conditional probability of Event $B$ given the occurrence of Event $A$ times the probability of Event $A$ occurring divided by the probability of Event $B$ occurring.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oRGjVvwX6M19"
   },
   "source": [
    "### Quiz 3.1\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "1.   Which of the following represents Bayes Theorem for the conditional probability of an Event $C$ occurring given the occurrence of Event $D$.\n",
    "\n",
    "        a) $P(C|D) = P(C|D)   P(C)   /   P(D)$\n",
    "        \n",
    "        b) $P(C|D) = P(D|C)   P(D)   /   P(C)$\n",
    "        \n",
    "        c) $P(C|D) = P(D|C)   P(C)   /   P(D)$\n",
    "        \n",
    "        d) $P(D|C) = P(C|D)   P(D)   /   P(C)$\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6cfSWS96KiIw"
   },
   "source": [
    "## Bayesian Inference\n",
    "\n",
    "---\n",
    "Let's reframe this in the context of human intuition. Bayesian inference can be thought of as using probabilistic intuition to predict an outcome. For instance, returning to the poke <a href='PP_2_Probability.ipynb#ex28'>Example 2.8</a>, let's say you always ask for 8 oz. of poke. However, you've observed that the last five times you bought poke from this shop, they have given you 8.5 oz. instead. From this prior experience, what quantity of poke would you expect to receive the next time you visited this shop and asked for 8 oz. of poke? You would likely reason that you would receive 8.5 oz. of poke, because you got an extra 0.5 oz the last 5 times you visited.\n",
    "\n",
    "Bayesian inference works the same way, using prior knowledge to predict future outcomes. To model the prior and future (posterior) outcomes, we use probability distributions that fit our scenario. These concepts are the key foundations of probabilistic programming. This is modelled mathematically using Bayes Theorem.\n",
    "\n",
    "Bayes Theorem can be reinterpreted for Bayesian inference as follows, where the *posterior distribution*, $P(A|B)$, is what we are trying to solve for given the *prior distribution* $P(A)$, the *likelihood*, $P(B|A)$, and the *model evidence*, $P(B)$. Another way to think about this is to consider $A$ equal to the *hypothesis* and $B$ equal to the *data*, where the data is known, just like Event $B$ was assumed to have already occurred. In other words, Event $A$ is conditional on Event $B$ occurring.\n",
    "\n",
    "\n",
    "<center><img src = \"media/chapter3/bayesian.png\" width = \"750\"></center>\n",
    "\n",
    "In the context of the poke problem above, Event $A$ refers to the quantity of poke received the next time you go to the poke shop and ask for 8 oz. of poke and Event $B$ refers to the previous times you've received 8.5 oz. of poke after asking for 8 oz. From this, your posterior $P(A|B)$ equals the probability of receiving 8.5 oz. of poke during your next visit *given* that you received 8.5 oz. of poke during your previous visits.\n",
    "\n",
    "<center><img src = \"media/chapter3/PP3_bayesianpoke.png\" width = \"750\"></center>\n",
    "\n",
    "The <a id='evidence'>model evidence</a>, $P(B)$ can be thought of as a way to normalize the likelihood and prior over the sample space. In practice, however, we will not be applying it in this module. Therefore, we can rewrite our formula for Bayesian inference to omit $P(B)$. Instead, <u>the posterior distribution is proportional to the likelihood times the prior distribution</u>, as follows ( where $\\propto$ means \"proportional to‚Äù):\n",
    "\n",
    "## <font color=\"red\">P(A|B)</font> $ \\propto $ <font color =\"#3FA9F5\">P(B|A)</font>  <font color = \"#7AC943\"> P(A) </font>\n",
    "\n",
    "or in other words, \n",
    "\n",
    "## <font color=\"red\">posterior</font> $\\propto $ <font color =\"#3FA9F5\">likelihood</font>   $\\times$ <font color = \"#7AC943\">prior</font>\n",
    "\n",
    "\n",
    "*   <a id='posterior'><b>Posterior Distribution</b></a>: The *posterior distribution* is what you are solving for - in other words, it is the new probability distribution given the probability of your hypothesis (the prior distribution) given your observed data (the likelihood).\n",
    "\n",
    "\n",
    "*   <a id='likelihood'><b>Likelihood</b></a>: The *likelihood* is not a probability distribution itself, but instead it is a function that relates the observed data to our hypothesis. It is proportional to a probability $P(B|A)$. The likelihood is sometimes represented as $\\theta$.\n",
    "\n",
    "*   <a id='prior'><b>Prior Distribution</b></a>: The *prior distribution* accounts for our preconceived knowledge about the distribution. In other words, the prior distribution represents the probability of the hypothesis occurring before running the experiment (i.e. before we have observable evidence).\n",
    "\n",
    "\n",
    "%% insert figure that shows how different prior distributions influence the posterior, holding the likelihood unchanged\n",
    "\n",
    "![reference](media/chapter3/priorstrength-1.png)\n",
    "\n",
    "\n",
    "Because the prior distribution can greatly influence the resulting posterior, it is very important to select a prior carefully.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gv7LU1cQ7-vy"
   },
   "source": [
    "### Quiz 3.2\n",
    "---\n",
    "\n",
    "1.   Which of the following represents the posterior for an Event $A$ given Event $B$?\n",
    "\n",
    "        a) $P(A|B)$\n",
    "        \n",
    "        b) $P(B|A)$\n",
    "        \n",
    "        c) $P(A,B)$\n",
    "        \n",
    "        d) $P(B)$\n",
    "        \n",
    "2. Given $P(B|A)$ and $P(A)$, what can one say about $P(A|B)$?\n",
    "        \n",
    "      a) $P(A|B)=P(B|A)P(A)$\n",
    "        \n",
    "      b) $P(A|B)=P(B|A)+P(A)$\n",
    "      \n",
    "      c) $P(A|B)$ is proportional to $P(B|A)P(A)$\n",
    "      \n",
    "      d) $P(A|B)$ is between $0$ and $1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hw-WisQOXFvj"
   },
   "source": [
    "# Prior Distribution\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "There are different kinds of prior distributions, and the prior one picks can greatly influence the model outcome or the posterior. <a id='hyperparameter'>*Hyperparameters*</a>  are the parameters of the prior distribution. Recall that the goal of probabilistic programming is to solve for the parameters of the posterior distribution, based off the the prior distribution and the likelihood function. Priors can be classified along a continuum from uniformative to informative.  There are pros and cons for using each type of prior, and the choice of an appropriate prior largely depends on the specific problem you are trying to solve. The degree to which the prior influences the posterior decreases as the sample size increases. \n",
    "\n",
    "## Uniformative priors\n",
    "\n",
    "<a id='uninf'>*Uniformative priors*</a>, or *flat priors*, do not make assumptions about preconceived knowledge and is oftentimes an optimal choice to use in probabilistic programming when there is insufficient previous knowledge about the prior distribution.\n",
    "\n",
    "### Example 3.1. \n",
    "---\n",
    "Recall the HPD <a href='PP_2_Probability.ipynb#HPD'>Example 2.9</a>:\n",
    "\n",
    "You are given the data that describes the number of HPD traffic incidents per day between July 15 and October 15 of 2018 (92 days total). Previous studies have shown that there is a sudden increase in the number of incidents during this period of time. A research group has determined that the average number of incidents before the increase is 186 and the average number of incidents after the sudden jump is 224. Your job is to determine on what day the increase happened.\n",
    "\n",
    "Recall that when we built a model for this example in the previous chapter, the prior was a variable *switch* that was generated with a uniform distribution with parameters $a=1$ and $b=92$. We didn't have any information about the day when *switch* happened, so we assumed that *switch* could happen any day with equal probability. This is an example of uninformative priors.\n",
    "\n",
    "## Informative priors\n",
    "\n",
    "An <a id='inf'>*informative prior*</a> assumes there is strong evidence for using preconceived knowledge and therefore should be used with caution. Informative priors are best reserved for cases where you have a large sample size, thus giving the prior less influence on the posterior.\n",
    "\n",
    "### Example 3.2.\n",
    "---\n",
    "Consider the same example about HPD incidents, but this time suppose the research group was able to determine that the switch happened in the second half of August. How would that affect the choice of distribution for prior *switch*?\n",
    "Since we now know some additional information, we can use it in our model. Because the jump in behavior happened between August 15 and August 31, which correspond to Day 31 and Day 47 respectively, the variable *switch* can be generated by the uniform distribution with parameters $a=31$ and $b=47$. This is an example of an informative prior.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5_awm7toGy34"
   },
   "source": [
    "### Quiz 3.3\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "1.   Which of the following snippets of code would generate an uninformative prior in PyMC3?\n",
    "\n",
    "        a) ```prior = pm.DiscreteUniform(\"prior\", 1, 100) ```\n",
    "        \n",
    "        b) ```prior = pm.discreteuniform(\"prior\", 1, 100) ```\n",
    "        \n",
    "        c) ```prior = pm.unif(\"prior\", 1, 100) ```\n",
    "        \n",
    "        d) ```prior = pm.Unif(\"prior\", 1, 100) ```\n",
    "        \n",
    "        \n",
    "2. Which of the following is true?\n",
    "\n",
    "      a) the prior has less influence over the posterior as the sample size increases\n",
    "      \n",
    "      b) the prior has greater influence over the posterior as the sample size increases\n",
    "      \n",
    "      c) the prior has less influence over the posterior as the sample size decreases\n",
    "      \n",
    "      d) the prior has no influence over the posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conjugate prior distributions\n",
    "\n",
    "The <a id='conj'>*conjugate prior distribribution*</a> refers to priors that are in the same exponential probability family as the likelihood function. Conjugate priors are easier and faster to calculate and thus are advantageous to use (with caution, as they inform the posterior) if they are applicable to the problem you are trying to solve.\n",
    "\n",
    "An example of a conjugate prior distribution is the beta distribution, which can be used with the binomial distribution. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nqWdb6ab_QwW"
   },
   "source": [
    "## Beta Distribution\n",
    "\n",
    "The <a id='beta'>*beta distribution*</a> is a continuous distribution that can be understood as representing a distribution of probabilities. It has two positive parameters, $\\alpha$ and $\\beta$:\n",
    "\n",
    "*   $\\alpha=$ shape parameter\n",
    "*   $\\beta=$ shape parameter\n",
    "\n",
    "The beta distribution with parameters $\\alpha=1$ and $\\beta=1$ is equivalent to continuous uniform distribution on $(0,1)$.\n",
    "\n",
    "To assign the continuous uniform distribution to a random variable `X` with PyMC3 package, use `X = pymc3.Beta('X', a, b)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DGNpQB0NHzVu"
   },
   "source": [
    "The beta probability density function looks graphically different when the parameters $\\alpha$ and $\\beta$ are changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lglGqtpUH9-R"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deabc1edad314448ad7f716e3c91c1ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.0, description='a', max=5.0, min=0.1), FloatSlider(value=1.0, descri‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# @hidden_cell\n",
    "\n",
    "%matplotlib inline\n",
    "import scipy as sp\n",
    "from scipy.stats import beta\n",
    "from ipywidgets import interactive\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def f(a = 1, b = 1):\n",
    "    x = np.arange(0, 1, .001)\n",
    "    pdf = beta.pdf(x, a, b)\n",
    "    fig5 = plt.figure()\n",
    "    plt.plot(x, pdf)\n",
    "    plt.title('Beta')\n",
    "    plt.show()\n",
    "\n",
    "interactive_plot = interactive(f, a = (0.1, 5, 0.1), b = (0.1, 5, 0.1))\n",
    "output = interactive_plot.children[-1]\n",
    "output.layout.height = '350px'\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-YcpZgqsKx4Q"
   },
   "source": [
    "### Quiz 3.4\n",
    "\n",
    "---\n",
    "\n",
    "1.   Determine parameters $\\alpha$ and $\\beta$ of the beta distribution so that the probabilities of getting something close to $0$ or $1$ are both extremely high comparing to something in the middle.\n",
    "\n",
    "        a) $\\alpha=0.6$, $\\beta=0.4$\n",
    "        \n",
    "        b) $\\alpha=0.5$, $\\beta=1.1$\n",
    "        \n",
    "        c) $\\alpha=1.1$, $\\beta=1.4$\n",
    "        \n",
    "        d) $\\alpha=2.6$, $\\beta=4.7$\n",
    "        \n",
    "\n",
    "\n",
    "2.   Select all the statements that are true for $X$ generated by the beta distribution with parameters $\\alpha=2$ and $\\beta=1$.\n",
    "        \n",
    "        a) $P(X=0)=0$\n",
    "        \n",
    "        b) $P(X=1)=1$\n",
    "        \n",
    "        c) $P(X=1)=2$\n",
    "        \n",
    "        d) as $X$ increases, so does $P(X)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "USJnqydIICt4"
   },
   "source": [
    "The beta distribution is often used as a prior for the binomial distribution, as is shown in the following example.\n",
    "\n",
    "\n",
    "<a id = 'ex33'></a>\n",
    "### Example 3.3. (McElreath‚Äôs book) \n",
    "---\n",
    "A seal is tossing a globe, catching it on the nose, and noting down if the globe comes down on water (W) or land (L).\n",
    "\n",
    "The seal tells us that the first 9 samples were:\n",
    "\n",
    "WLWWWLWLW.\n",
    "\n",
    "We wish to understand the evolution of belief in the fraction of water on earth as the seal tosses the globe.\n",
    "\n",
    "The outcomes after tossing a globe $9$ times are represented by the Binomial distribution with parameters $n=9$ and some probability that the the globe will come down on water $p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R2rJfvGI_TdC"
   },
   "source": [
    "In this example, the <font color=\"red\">posterior</font> and the <font color = \"#7AC943\"> prior  </font> distributions are represented by the beta distribution and the <font color =\"#3FA9F5\"> likelihood</font> function is represented by the binomial distribution as shown below.\n",
    "\n",
    "\n",
    "<center><img src = \"media/chapter3/bayesian_betabinom.png\" width = \"750\"></center>\n",
    "\n",
    "The binomial and beta distributions are both parameterized (specifically, the parameter of the binomial distribution and the hyperparameter of the beta distribution) by a probability $p()$.\n",
    "\n",
    "Given the data observed from the binomial distribution $Binomial(n,p)$, where $p$ is a prior parameter generated by $Beta(\\alpha,\\beta)$. And given that the observed data has $k$ successes and $n-k$ failures, then the posterior distribution is the beta distribution with new parameters $\\alpha+k$ and $\\beta+n-k$ (increase parameter $\\alpha$ by the number of successes and parameter $\\beta$ by the number of failures).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5TRGh7G2sYvQ"
   },
   "source": [
    "Assume that at first we don't know anything about the fraction of water on earth, then we start with the prior distribution $p=Beta(1,1)$. Recall that $Beta(1,1)$ is equivalent to the continuous uniform distribution on the interval $(0,1)$. \n",
    "\n",
    "Since $p$ is the probability that the globe will come down on water, we think about water as a success and land as a failure. The observed data in the example is WLWWWLWLW, so there are 6 successes and 3 failures. Therefore, the posterior distribution is the beta distribution with parameters $\\alpha=1+6=7$ and $\\beta=1+3=4$.\n",
    "\n",
    "The following code shows the change of belief in the fraction of water on earth. Here we are using probability density function for beta distribution `beta.pdf()` from `scipy.stats` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 362
    },
    "colab_type": "code",
    "id": "dxbjBewUrSAc",
    "outputId": "d5270511-21f5-4419-8c0b-0c00120e1452"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.stats import beta\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# define the hyperparameters\n",
    "a = 1\n",
    "b = 1\n",
    "\n",
    "# x values from 0 to 1 \n",
    "x = np.arange(0, 1, .001)\n",
    "\n",
    "# generate the prior distribution\n",
    "pdf_prior = beta.pdf(x, a, b)\n",
    "\n",
    "#observed data\n",
    "data = \"WLWWWLWLW\"\n",
    "\n",
    "#add the number of successes(W) to a\n",
    "a += data.count(\"W\")\n",
    "\n",
    "#add the number of failures(L) to b\n",
    "b += data.count(\"L\")\n",
    "\n",
    "#generate the posterior distribution\n",
    "pdf_posterior = beta.pdf(x, a, b)\n",
    "\n",
    "# plot the distribution\n",
    "fig6 = plt.figure()\n",
    "plt.plot(x, pdf_prior)\n",
    "plt.plot(x, pdf_posterior)\n",
    "plt.legend([\"prior\",\"posterior\"])\n",
    "plt.title('Beta')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wt6FuOdxrF4a"
   },
   "source": [
    "  Assume that instead of telling us all 9 samples at once, the seal is telling us the outcomes one by one. In the end, the belief (posterior distribution) will be the same as when we are just given 9 samples. But this way we can keep track of the evolution in belief (see how posterior distribution is changing with every new outcome)\n",
    "\n",
    "![alt text](media/chapter3/steps_globe.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aMNPO5FqOt8u"
   },
   "source": [
    "### Quiz 3.5\n",
    "\n",
    "---\n",
    "\n",
    "1.   Let's say in the above <a href='#ex33'>Example 3.3</a> the seal tossed the globe and the first nine samples fell on water (W) or land (L) as follows: LLWLWLWLW. Based on these nine samples, which of the following are the best parameters for the beta distribution that describes updated belief?\n",
    "\n",
    "        a) $\\alpha=6$ and $\\beta = 3$\n",
    "        \n",
    "        b) $\\alpha=4$ and $\\beta = 5$\n",
    "        \n",
    "        c) $\\alpha=3$ and $\\beta = 6$\n",
    "        \n",
    "        d) $\\alpha=5$ and $\\beta = 4$\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C2CWlLEmfbgV"
   },
   "source": [
    "%%make this table a hidden portion\n",
    "\n",
    "Fortunately, the distributions we discussed in Chapter 2 all have conjugate priors:\n",
    "\n",
    "<table style=\"width:90%\" align=\"center\">\n",
    "  <tr>\n",
    "    <td><b><center>Likelihood</center></b></td>\n",
    "    <td><b><center>Parameters</center></b></td>\n",
    "    <td><b><center>Conjugate Prior</center></b></td>\n",
    "    <td><b><center>Hyperparameters</center></b></td>\n",
    "    <td><b><center>Posterior</center></b></td>\n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td><center>Binomial</center></td>\n",
    "    <td><center>probability ($p$)</center></td>\n",
    "    <td><center>[Beta](https://en.wikipedia.org/wiki/Beta_distribution)</center></td>\n",
    "    <td><center>$\\alpha$ = number of successes $+ 1$, $\\beta$ = number of failures $+1$</center></td>\n",
    "    <td><center>Beta</center></td>\n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td><center>Poisson</center></td>\n",
    "    <td><center>$\\lambda$</center></td>\n",
    "    <td><center>[Gamma](https://en.wikipedia.org/wiki/Gamma_distribution)</center></td>\n",
    "    <td><center>$k$ = number of occurrences, $\\theta$ = inverse of the number of intervals</center></td>\n",
    "    <td><center>Gamma</center></td>\n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td><center>Gaussian (if $\\mu$ is known, and $\\sigma^2$ is unknown)</center></td>\n",
    "    <td><center>$\\sigma^2$</center></td>\n",
    "    <td><center>[Inverse gamma](https://en.wikipedia.org/wiki/Inverse-gamma_distribution)</center></td>\n",
    "    <td><center>$2\\alpha$ = number of observations with variance  $\\beta$ $/$ $\\alpha$, $\\beta$ = sum of squared deviations from known $\\mu$</center></td>\n",
    "    <td><center>Inverse gamma</center></td>\n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td><center>Gaussian (if $\\sigma^2$ is known, and $\\mu$ is unknown)</td>\n",
    "    <td><center>$\\mu$</td>\n",
    "    <td><center>[Gaussian](https://en.wikipedia.org/wiki/Normal_distribution)</td>\n",
    "    <td><center>$\\mu_0$ = mean, $\\sigma^2_0$ = known variance</td>\n",
    "    <td><center>Gaussian</td>\n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td><center>Uniform (continuous)</center></td>\n",
    "    <td><center>$U (0,\\theta$)</center></td>\n",
    "    <td><center>[Pareto](https://en.wikipedia.org/wiki/Pareto_distribution)</center></td>\n",
    "    <td><center>$x_m$ = maximum value, $k$ = number of observations</center></td>\n",
    "    <td><center>Pareto</center></td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "%% end hidden section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qerqna9BRm5n"
   },
   "source": [
    "## Finding the posterior\n",
    "---\n",
    "\n",
    "Let's go back to the HPD incidents <a href='PP_2_Probability.ipynb#HPD'>Example 2.9</a> we introduced before. From Chapter 2 we know how to write the model that describes the experiment in Python as well as understand how Bayesian inference works. But how can one implement inference in Python?\n",
    "\n",
    "In this chapter we introduce several types of inference that can be used to find the posterior.\n",
    "\n",
    "We have already used one of the inference methods in the Introduction - rejection sampling. Recall, that in order to perform rejection sampling, one generates data and if it coincides with observed data, we keep information about the prior. After repeating the process many times, one has set of priors that generated observed data, i.e. the posterior. \n",
    "\n",
    "Although such an algorithm is easy to understand and implement, it is inefficient and impractical: a lot of rejections will take place before a sample that fits observed data is generated. Because of its inefficiency, rejection sampling is not implemented in PyMC3, and we will not discuss this further. \n",
    "\n",
    "There are two major types of methods of inference (*Markov-Chain Monte-Carlo* (MCMC) and non-MCMC) to determine the posterior distribution. MCMC methods preferentially sample from the areas of a probability distribution with a greater chance of success. This is compared to non-MCMC methods, which do not account for probability in their sampling density.\n",
    "\n",
    "Some non-MCMC methods are often too simple to use with complicated problems, but can be useful. One example of a non-MCMC method is grid approximation. Grid approximation is useful in the case of a one-dimensional problem (i.e. our model only has one parameter), but becomes computationally expensive when the model has multiple parameters.\n",
    "\n",
    "Grid approximation can be intuitively explained using a grid (or table). Imagine you have a grid that has $n$ boxes, and each one of these initially boxes represents the prior distribution. For instance, for the uniformative prior, these grid boxes would all have the same value. Building on this, each grid box also contains information about the likelihood. To solve for the posterior, the posterior is calculated for each grid box using Bayes Theorem, which arrives at the posterior distribution.  \n",
    "\n",
    "Let's try an example, returning to our HPD incidents problem. We will first solve with grid approximation and then with one of the MCMC algorithms and compare the outcomes.\n",
    "\n",
    "How would we solve the problem using the grid approximation method?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bPetv2jAhmGQ"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "\n",
    "HPD_data = np.array([183, 181, 155, 177, 193, 174, 187, 181, 178, 196, \n",
    " 187, 171, 181, 194, 178, 201, 183, 192, 212, 159, 179, 195, 190, 206, \n",
    " 171, 195, 177, 184, 202, 175, 173, 195, 207, 216, 225, 246, 197, 250, \n",
    " 217, 250, 229, 230, 216, 239, 222, 224, 231, 225, 204, 227, 237, 225, \n",
    " 211, 209, 235, 241, 209, 196, 246, 215, 241, 238, 233, 235, 202, 214, \n",
    " 225, 230, 187, 220, 234, 206, 228, 234, 242, 189, 250, 233, 225, 246, \n",
    " 224, 211, 201, 217, 219, 224, 225, 251, 222, 220, 222, 202])\n",
    "\n",
    "# define a function for grid approximation with 92 points, and HPD_data\n",
    "def posterior_gridapprox(grid_points = 92, data = HPD_data, \n",
    "                         lam_1 = 186, lam_2 = 224):\n",
    "  \n",
    "  # setup grid with grid_points points\n",
    "  grid = np.arange(1, grid_points + 1)   \n",
    "  \n",
    "  # prior is uniform (uninformative)\n",
    "  prior = stats.randint.pmf(grid, 1, grid_points + 1)\n",
    "  # change the parameters of uniform distribution for informative prior\n",
    "  # prior = stats.randint.pmf(grid, 31, 47 + 1)\n",
    "  \n",
    "  # lam with switch\n",
    "  day = np.arange(1, grid_points + 1)  \n",
    "  lam = np.array([lam_1*(day < i)+lam_2*(day>=i) for i in grid])\n",
    "  \n",
    "  # use the poisson for the likelihood\n",
    "  likelihood = np.array([np.prod(stats.poisson.pmf(data, lam[i])) \n",
    "                         for i in range(grid_points)])\n",
    "  \n",
    "  # posterior is proportional to the likelihood times the prior\n",
    "  unstd_posterior = likelihood * prior\n",
    "  \n",
    "  # standardize the posterior\n",
    "  posterior = unstd_posterior / unstd_posterior.sum()\n",
    "  \n",
    "  # return results\n",
    "  return grid, likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jIqMMySFJENa"
   },
   "source": [
    "Now let's do the same problem using PyMC3 and an MCMC (Metropolis-Hastings) approach.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "## MCMC\n",
    "\n",
    "<a id='mcmc'>Markov Chain Monte Carlo</a> (MCMC) methods are a family of algorithms that perform a random walk from one position to another given that the next step is an improvement over the previous step. Most of such algorithms can be described the following way:\n",
    "\n",
    "\n",
    "\n",
    "1.   Pick a starting position\n",
    "2.   Propose to move to a new position according to some proposal distribution.\n",
    "3.   Accept/reject the new position based on the prior distribution and observed data.\n",
    "4.     *   If accepted, move to the proposed position\n",
    "    *   If rejected, do not move to the proposed position\n",
    "5.  Return to Step 2.\n",
    "\n",
    "One of MCMC algorithms is called Metropolis-Hastings. In the contrast to rejection sampling or grid approximation, Metropolis-Hastings is implemented in PyMC3 package. Let's look how it works in the mentioned before <a href='PP_2_Probability.ipynb#HPD'>Example 2.9</a>. \n",
    "\n",
    "\n",
    "\n",
    "First, we import the PyMC3 package.\n",
    "\n",
    "```\n",
    "import pymc3 as pm\n",
    "\n",
    "```\n",
    "Then we need to create the model within the context of Model object.\n",
    "\n",
    "```\n",
    "with pm.Model() as HPD_model:\n",
    "    switch = pm.DiscreteUniform(\"switch\", 1, 92)\n",
    "    lam_1 = 186\n",
    "    lam_2 = 224\n",
    "    day = np.arange(1, 92+1)\n",
    "    lam = pm.math.switch(day < switch, lam_1, lam_2)\n",
    "    observation = pm.Poisson(\"obs\", lam, observed = HPD_data)\n",
    "```\n",
    "Then, we choose Metropolis-Hastings to determine the posterior (within the context of Model object).\n",
    "\n",
    "```\n",
    "step = pm.Metropolis()\n",
    "```\n",
    "The next line of code performs the step method of our choice (Metropolis-Hastings) by sampling chosen number of points (20000) using the algorithm above.\n",
    "\n",
    "\n",
    "```\n",
    "trace = pm.sample(20000, step=step)\n",
    "```\n",
    "\n",
    "Since first steps will contain evidence of the initial condition, you always want to eliminate $5-10\\%$ of your samples.\n",
    "\n",
    "```\n",
    "burned_trace=trace[1000:]  \n",
    "```\n",
    "Finally, we can look at the posterior distribution.\n",
    "\n",
    "\n",
    "```\n",
    "switch_samples = burned_trace[\"switch\"]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "PQLxbS-n2Nza",
    "outputId": "7f515382-bf6f-424f-effb-5834fbf1245a"
   },
   "outputs": [],
   "source": [
    "import pymc3 as pm\n",
    "import numpy as np\n",
    "\n",
    "HPD_data = [183, 181, 155, 177, 193, 174, 187, 181, 178, 196, 187, 171,\n",
    " 181, 194, 178, 201, 183, 192, 212, 159, 179, 195, 190, 206, 171, 195,\n",
    " 177, 184, 202, 175, 173, 195, 207, 216, 225, 246, 197, 250, 217, 250,\n",
    " 229, 230, 216, 239, 222, 224, 231, 225, 204, 227, 237, 225, 211, 209,\n",
    " 235, 241, 209, 196, 246, 215, 241, 238, 233, 235, 202, 214, 225, 230, \n",
    " 187, 220, 234, 206, 228, 234, 242, 189, 250, 233, 225, 246, 224, 211,\n",
    " 201, 217, 219, 224, 225, 251, 222, 220, 222, 202]\n",
    "\n",
    "with pm.Model() as HPD_model:\n",
    "  #note that we are using uninformative prior, \n",
    "  #     but could also choose informative prior instead\n",
    "  switch = pm.DiscreteUniform(\"switch\", 1, 92)\n",
    "  lam_1 = 186\n",
    "  lam_2 = 224\n",
    "  day = np.arange(1, 92+1)\n",
    "  lam = pm.math.switch(day < switch, lam_1, lam_2)\n",
    "  observation = pm.Poisson(\"obs\", lam, observed = HPD_data)\n",
    "  \n",
    "  # use Metropolis-Hastings MCMC method to determine the posterior\n",
    "  step = pm.Metropolis()\n",
    "  \n",
    "  # sample 20000 points\n",
    "  trace = pm.sample(20000, step = step)\n",
    "  \n",
    "  # set a burn-in period of 1000 points\n",
    "  burned_trace = trace[1000:]  \n",
    "\n",
    "# posterior distribution\n",
    "switch_samples = burned_trace[\"switch\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_cXEp1wK-0sS"
   },
   "source": [
    "In this example, the resulting posterior distribution is similar using both approaches: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "colab_type": "code",
    "id": "dHMV71AH7TFr",
    "outputId": "5cb1115f-4c3d-4b95-927f-f08cbd6dfbda"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot the posterior distribution using Metropolis and grid approximation\n",
    "# plot of the posterior using grid approximation\n",
    "ax = plt.subplot(121)\n",
    "\n",
    "plt.xlim(29,37)\n",
    "grid, posterior = posterior_gridapprox()\n",
    "plt.plot(grid, posterior, 'o-')\n",
    "\n",
    "plt.xlabel('switch')\n",
    "plt.ylabel('p(switch)')\n",
    "plt.title('Grid Approximation')\n",
    "\n",
    "\n",
    "\n",
    "# plot of the posterior distribution using Metropolis-Hastings\n",
    "ax = plt.subplot(122)\n",
    "\n",
    "plt.xlim(29,37)\n",
    "\n",
    "plt.hist(switch_samples, histtype='stepfilled', bins=25, \n",
    "         color=\"#A60628\", normed=True)\n",
    "plt.title('Metropolis-Hastings')\n",
    "plt.xlabel('switch')\n",
    "ax.set_yticklabels([])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jdRgzRtj-yfr"
   },
   "source": [
    "This may be the case for a simple problem, such as the HPD incidents example (i.e. the value for only one parameter is being determined), however for more complicated problems, grid approximation is too computationally expensive and an MCMC method is more appropriate to use.\n",
    "\n",
    "\n",
    "Both methods showed that the switch happened between Day 32 and Day 35 (which corresponds to August, 16 - 19) with the highest probability of happening on Day 33 (August 17). Coincidentally, August 17 also coincides with UH students arriving the week before semester started. These results imply that there is a chance that the beginning of Fall semester at UH affects the number of incidents on the road. Note, that we are using toy data in this example, therefore it does not represent real number of incidents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lPdXgy-Go66x"
   },
   "source": [
    "Other step-methods implemented in PyMC3 are: NUTS, Slice, HamiltonianMC. Try to use them instead of Metropolis method and see if it affects your results.\n",
    "\n",
    "Also note, that if one doesn't specify the step to use, PyMC3 will choose the step itself in an optimal way. So, instead of writing\n",
    "\n",
    "\n",
    "```\n",
    "step = pm.Metropolis()\n",
    "trace = sample(20000, step = step)\n",
    "```\n",
    "\n",
    "one could just use the following:\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "trace = sample(20000)\n",
    "\n",
    "```\n",
    "It is a good idea to use the latter when you are not sure what step or combination of steps would work best for the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-0vGGvbhTdTd"
   },
   "source": [
    "### Quiz 3.6\n",
    "\n",
    "---\n",
    "\n",
    "1.  True or False: It is important to set `burned_trace` in the context of your model because it takes some time for PyMC3 to start sampling from the target distrubtion and thus the first elements of a trace are highly impacted by the initial conditions of your model.\n",
    "\n",
    "      a) True\n",
    "      \n",
    "      b) False\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wAD4PXbLpDiA"
   },
   "source": [
    "In the next chapter we are going to look at several common types of problems and build models for them. Also we will learn how to check if your model is a good choice for the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vEsmUlKhpIOU"
   },
   "source": [
    "## <u>Definitions</u>\n",
    "\n",
    "<a href='#conditional'>**conditional probability**</a>: the likelihood that an event will occur given the occurrence of a different event. We can refer to these events as Event $A$ and Event $B$, where Event $B$ has already occurred and we are interested in the probability that Event $A$ will occur in addition to Event $B$. The conditional probability of Event $A$ occurring given (where $|$ represents \"given\") the occurrence of Event $B$ is denoted as: $P(A|B)$. \n",
    "\n",
    "<a href='#Bayes'>**Bayes Theorem**</a>: a way to calculate the conditional probability. The conditional probability of Event $A$ occurring given the occurrence of Event $B$ is equal to the conditional probability of Event $B$ given the occurrence of Event $A$ times the probability of Event $A$ occurring divided by the probability of Event $B$ occurring.\n",
    "\n",
    "<a href='#evidence'>**model evidence (P(B))**</a>: a way to normalize the likelihood and prior over the sample space.\n",
    "\n",
    "<a href='#prior'>**prior distribution (P(A))**</a>: represents the probability of the hypothesis occurring before running the experiment (i.e. before we have observable evidence).\n",
    "\n",
    "<a href='#uninf'>**uninformative prior (flat prior)**</a>: a prior that does not make assumptions about preconceived knowledge.\n",
    "\n",
    "<a href='#inf'>**informative prior**</a>: a prior that assumes there is a strong basis of evidence for preconceived knowledge.\n",
    "\n",
    "<a href='#conj'>**conjugate prior**</a>: a prior that is in the same exponential probability family as the likelihood function.\n",
    "\n",
    "<a href='#hyperparameter'>**hyperparameter**</a>: parameters of the prior distribution.\n",
    "\n",
    "<a href='#posterior'>**posterior distribution (P(A|B))**</a>:  the new probability distribution given the probability of the hypothesis (the prior distribution) given the observed data (the likelihood).\n",
    "\n",
    "<a href='#likelihood'>**likelihood ($\\theta$)**</a>: a function that relates the observed data to a hypothesis. It is proportional to a probability $P(B|A)$. \n",
    "\n",
    "<a href='#beta'>**beta distribution**</a>:  a continuous probability distribution with two positive-valued shape parameters, $\\alpha$ and $\\beta$:\n",
    "\n",
    "<a href='#mcmc'>**Markov-Chain Monte-Carlo (MCMC)**</a>:  methods of inference to determine the posterior distribution by preferentially sampling from areas of a probability distribution with a greater chance of success.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7Bj_Vpdyi0Eh"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "PP_3_Bayesian.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
