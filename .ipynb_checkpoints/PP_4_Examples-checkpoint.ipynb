{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "evoo74QF9IPz"
   },
   "source": [
    "# Chapter 4. Alternative Methods, Examples, and Model Checking\n",
    "\n",
    "---\n",
    "In the previous chapters, we have discussed fundamental concepts behind probabilistic programming and probability and introduced several methods of finding the posterior distribution. In this chapter, we are going to introduce several additional methods of finding the posterior, as well as methods for checking the validity of your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MBbUW5SmHKt1"
   },
   "source": [
    "## Hierarchical models\n",
    "---\n",
    "\n",
    "A <a id='hier'>*hierarchical (or multi-level) model*</a> can be used to model <a id='latent'>*latent variables*</a>  (i.e. what you are trying to model through inference, as opposed to observable variables). Examples of latent variables include how healthy or happy an individual is - such variables cannot be observed directly, but can be inferred computationally. While health itself cannot be directly quantified, predictors of health (such as number of doctor's visits per year, days of exercise per week, or servings of vegetables eaten per day) can be quantified.\n",
    "\n",
    "Hierarchical models are useful for inferring the posterior when one has a more complicated problem. In previous examples, we have assumed that our observations are independent of one another. In this chapter, we will now consider the alternative case where observations are dependent on one another. A hierarchical model can be used to answer the following kinds of questions:\n",
    "\n",
    "- what grade will a student/all of the students in the class get on a final exam given their grade in the class before the final?\n",
    "- for a team sport, how does an individual athlete's performance affect the team's overall performance?\n",
    "\n",
    "A hierarchical model uses inferred dependencies (with a multi-layered, or hierarchical approach) to model the posterior. Recall, we previously mentioned the concept of *hyperparameters*, or the parameters of the prior distribution, which is used when making hierarchical models. The priors of the hyperparameters are called <a id ='hyperprior'>*hyperpriors*</a>.\n",
    "\n",
    "\n",
    "%% insert generative model diagram \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VWSh5WblHTnl"
   },
   "source": [
    "### Example 4.1\n",
    "\n",
    "\n",
    "There are many coffee plantations in the state of Hawaiʻi. The [coffee borer beetle](https://en.wikipedia.org/wiki/Coffee_borer_beetle) is a problem for coffee growers worldwide and can cause coffee plants to die. Note that the problem we are going to consider does not represent the true behavior of the coffee borer beetle and is simplified for the purposes of this example.\n",
    "\n",
    "We want to analyze the infection rate of four Hawaiian islands: Kaua'i, O'ahu, Maui and Hawai'i. Each island has coffee plantations (5 on Kauaʻi, 8 on Oʻahu, 7 on Maui, 10 on Hawaiʻi) and each plantation is divided into sections of the same area (the ratios between the area of the section and the area of entire plantation are given in the problem). Sections are created in order to prevent beetles from spreading over the plantation. This means that if a beetle gets into one section, then all the plants in this section will be affected soon afterwards, but the neighboring sections will not be impacted. Therefore, each section is considered either infected or not infected.\n",
    "\n",
    "<center><img src = \"media/chapter4/coffee.jpg\" width = \"450\"></center>\n",
    "\n",
    "\n",
    "To analyze the infection rate, we randomly choose plantations on each island and sample $50$ sections from each plantation. We also have the ratio of the section area to the plantation area for each of the chosen plantations, because it might be used in the generative model.\n",
    "\n",
    "We are given the following data:\n",
    "\n",
    "\n",
    "*   `observed_data` = an array of integers between $0$ and $50$ that represent the number of infected areas for each of the chosen plantations\n",
    "\n",
    "*   `ratio` = an array of numbers between $0$ and $1$ that represent the ratio of area of the section to the area of plantation\n",
    "\n",
    "* `islands` = an array of characters ('H', 'M' , 'O', 'K'), representing the island where the chosen plantation is located, i.e. Hawai'i, Maui, O'ahu, and Kaua'i, .\n",
    "\n",
    "The first step in solving the problem, as usual, is to create a generative model. Since the generative model in this problem has several layers, we are going start from a simple case and add the layers gradually.\n",
    "\n",
    "To start, let's consider just one plantation. We need to sample $50$ sections from the plantation and determine how many of them are infected. What distribution can we use to generate the number of infected sections? We can use the binomial distribution with a parameter $n=50$ and some parameter $p$. The following code generates this smaller model in PyMC3:\n",
    "\n",
    "\n",
    "```\n",
    "with pm.Model() as coffee_plantation_model:\n",
    "    infected = pm.Binomial(\"infected\", n=50, p=p, observed = observed_data)\n",
    "```\n",
    "In the context of the smaller problem, $p$ is our prior, which can be generated using the continuous uniform distribution with parameters $a=0$ and $b=1$ if we don't have any additional information about the plantation. But there is more information about $p$ from our initial problem. Recall that we are also given the `ratio`, which is useful because we are currently only considering one plantation. How can different ratios change the rate of infection?  If just one beetle arrives in the section of plantation with a ratio of $1/100$, then it almost immediately will infect $1/100$ of plantation. If, instead, the ratio of the same plantation is $1/4$, then one beetle would quickly infect quarter of the plants. So, we need to take into account the fact that the smaller the ratio is, the less damage beetle will cause to the plantation. We also are going to assume that although the sections help from spreading the beetle over the whole plantation, it is likely that the beetle won't be alone, and others might affect other sections. So $p$ will more likely be something close to $0$ or $1$ rather then $0.5$. The last thing we are going to assume, is that plantation can be in a good shape or in a bad shape. That means that if it's in a good shape, then it has more chances of not getting completely infected by the beetles than of getting infected and vice versa.\n",
    "\n",
    "Recall, that the distribution that is used to parameterize for the Binomial is the Beta distribution with parameters $a$ and $b$. Also recall that if $a<1$ and $b<1$, then the outcome is more likely to be close to $0$ or $1$, respectively, that is exactly what we are looking for. We also know that $a>b$ gives higher probability of getting value close to $1$, and $a<b$ gives higher probability of getting outcome close to $0$. \n",
    "\n",
    "<center><img src = \"media/chapter4/betadist.jpg\" width = \"450\"></center>\n",
    "\n",
    "Considering the plantation's ratio, we can choose parameters for the beta distribution the following way:\n",
    "\n",
    "\n",
    "*   if a plantation is in a good shape: $a=1/4 -$`ratio`, $b=3/4+$`ratio` ;\n",
    "*   if a plantation is in a bad shape: $a=3/4+$`ratio` , $b=1/4-$`ratio`;\n",
    "\n",
    "This is not the only choice of $a$ and $b$ that would work, but it satisfies all the conditions mentioned above, therefore, it is a good choice for us.\n",
    "\n",
    "The choice of good/bad shape can be generated using the Bernoulli distribution with some parameter $I$.\n",
    "\n",
    "With the above adjustments, the generative model can be implemented the following way:\n",
    "\n",
    "\n",
    "```\n",
    "with pm.Model() as coffee_plantation_model:\n",
    "    shape = pm.Bernoulli(\"pl\", p = I)\n",
    "    a = pm.math.switch(shape < 1/2, 1/4 - ratio, 3/4 + ratio)\n",
    "    b = pm.math.switch(shape < 1/2, 3/4 + ratio, 1/4 - ratio)\n",
    "    p = Beta(\"p\", alpha = a, beta = b)\n",
    "    infected = pm.Binomial(\"infected\", n=50, p=p, observed = observed_data)\n",
    "```\n",
    "\n",
    "Suppose we also want to take into account the island the considered plantation is from. We know from the original problem that each island has some infection rate. We can say that this rate affects whether the plantation is in a good or bad shape. Therefore, the parameter $I$ is exactly what we want to infer. Since we don't know anything about it, we can make it an uninformative prior, that is to generate it using the uniform continuous distribution with parameters $a=0$ and $b=1$. So, the finished model for one plantation will be the following:\n",
    "\n",
    "```\n",
    "with pm.Model() as coffee_plantation_model:\n",
    "    I = pm.Uniform(\"I\", a = 0, b = 1)\n",
    "    shape = pm.Bernoulli(\"pl\", p = I)\n",
    "    a = pm.math.switch(shape < 1/2, 1/4 - ratio, 3/4 + ratio)\n",
    "    b = pm.math.switch(shape < 1/2, 3/4 + ratio, 1/4 - ratio)\n",
    "    p = Beta(\"p\", alpha = a, beta = b)\n",
    "    infected = pm.Binomial(\"infected\", n=50, p=p, observed = observed_data)\n",
    "```\n",
    "\n",
    "The last step is to transform the above model into a model that has data about more than one plantation.  The following image summarizes the generative model for the problem for four islands and more than one plantation per island as described above.\n",
    "\n",
    "\n",
    "<center><img src = \"media/chapter4/coffee_model.jpg\" width = \"650\"></center>\n",
    "\n",
    "\n",
    "This can be modelled in PyMC3 in the following way:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5RPiz6EsHYJ1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(435)\n",
    "#data for hierarchical\n",
    "H = 10\n",
    "M = 7\n",
    "O = 8\n",
    "K = 5\n",
    "sample_size = 50\n",
    "H_ = 0.56\n",
    "M_ = 0.32\n",
    "O_ = 0.78\n",
    "K_ = 0.11\n",
    "islands = np.array([\"H\"]*H+[\"M\"]*M+[\"O\"]*O+[\"K\"]*K)\n",
    "rates = np.array([H_]*H+[M_]*M+[O_]*O+[K_]*K)\n",
    "ratio = 1/np.random.randint(10,100,len(islands))\n",
    "binom = np.array([np.random.binomial(n=1,p=p_rat,size=1)[0] for p_rat in rates])\n",
    "opt1=3/4+ratio\n",
    "opt2 =1/4-ratio\n",
    "a = binom*(opt1)+(1-binom)*opt2\n",
    "b = binom*opt2+(1-binom)*(opt1)\n",
    "beta = np.array([np.random.beta(a[i],b[i],size=1)[0] for i in range(len(a))])\n",
    "observed_data = np.array([np.random.binomial(p = beta_, n = sample_size, size=1)[0]for beta_ in beta])\n",
    "#grid\n",
    "#plt.plot(np.arange(0,30),grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "BICJN7JlHcwM",
    "outputId": "24afcd9b-9d29-44f7-d797-1d1b2cbcd14c"
   },
   "outputs": [],
   "source": [
    "# Hierarchical model\n",
    "\n",
    "import pymc3 as pm\n",
    "import numpy as np\n",
    "\n",
    "#islands= np.array(['H', 'H','H','M','O','M','O','K','O','K','K','M','M'])\n",
    "#ratio = np.array([1, 1,1,2,3,2,3,4,3,4,4,2,2])*1/100\n",
    "#observed_data = np.array([0, 0,1,5,6,4,9,4,3,4,4,2,2])\n",
    "sample_size = 50\n",
    "\n",
    "N = len(observed_data)\n",
    "\n",
    "with pm.Model() as coffee_plantation_model:\n",
    "  H = pm.Uniform(\"H\", 0, 1)\n",
    "  M = pm.Uniform(\"M\", 0, 1)\n",
    "  O = pm.Uniform(\"O\", 0, 1)\n",
    "  K = pm.Uniform(\"K\", 0, 1)\n",
    "  \n",
    "  I = pm.math.switch(islands == 'H', H,\n",
    "                     pm.math.switch(islands == 'M', M,\n",
    "                                    pm.math.switch(islands == 'O', O, K)))\n",
    "  shape = pm.Bernoulli(\"shape\", I, shape = N)\n",
    "  a = pm.math.switch(pm.math.lt(shape, 1/2), 1/4 - ratio, 3/4 + ratio)\n",
    "  b = pm.math.switch(pm.math.lt(shape, 1/2), 3/4 + ratio, 1/4 - ratio)\n",
    "  #pm.math.eq(shape, 0)\n",
    "  p = pm.Beta(\"p\", a, b, shape = len(islands))\n",
    "  Grid = pm.Binomial(\"grid\", p = p, n = sample_size, \n",
    "                     shape = N, observed = observed_data)\n",
    "  trace = pm.sample(20000)\n",
    "  burned_trace = trace[1000:]\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "colab_type": "code",
    "id": "Px1oPbBYHg_w",
    "outputId": "c14b80be-fb41-48d6-a4d9-8db0453eaa9f"
   },
   "outputs": [],
   "source": [
    "pm.plot_posterior(burned_trace, varnames=['H', 'M', 'O', 'K'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6_AgkWo6HvWv"
   },
   "source": [
    "Our hierarchical model shows that on average, 59.3%, 34.8%, 71.4%, and 21.4% of plantations are potentially infected for the islands of Hawai'i, Maui, O'ahu, and Kaua'i, respectively. \n",
    "\n",
    "The <a id='hpd'>*95% Highest Posterior Density (HPD)*</a>  represents the likely range of values within a 95% confidence interval for the posterior distribution (its range is indicated by the black bar in the figure above). In other words, the HPD can be thought of the range of values for the posterior of a modelled parameter with a p = 0.95. \n",
    "\n",
    "The HPD for our hierarchical model ranges from 0% plantation infection rate (on all islands) to 100% on O'ahu, given our model inputs. \n",
    "\n",
    "In summary:\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><b><center>Island</center></b></td>\n",
    "    <td><center>Hawai'i</center></td>\n",
    "    <td><center>Maui</center></td>\n",
    "    <td><center>O'ahu</center></td>\n",
    "    <td><center>Kaua'i</center></td>\n",
    "  </tr>\n",
    "  <tr>    \n",
    "    <td><b><center>Average % Infected</center></b></td>\n",
    "    <td><center>59.3%</center></td>\n",
    "    <td><center>34.8%</center></td>\n",
    "    <td><center>71.4%</center></td>\n",
    "    <td><center>21.4%</center></td>\n",
    "  </tr>\n",
    "   <tr>    \n",
    "    <td><b><center>95% HPD</center></b></td>\n",
    "    <td><center>28.8 - 91.3%</center></td>\n",
    "    <td><center>3.8 - 66.2%</center></td>\n",
    "    <td><center>32 - 100%</center></td>\n",
    "    <td><center>0 - 56.8%</center></td>\n",
    "  </tr>\n",
    "  </table>\n",
    "    \n",
    "\n",
    "The hierarchical approach allowed us to look at each island separately, as well as look at the effects at the plantation and island scale. As you can see from the 95% HPD, the model results in significant overlap in infection rates between all islands.\n",
    "\n",
    "Hierarchical models are generally considered more robust than non-hierarchical approaches because of the linkage between the model components through the hyperpriors. This linkage (sometimes referred to as *shrinkage*) means the model and resulting posteriors are less likely to be skewed by outliers and thus is an improvement creating multiple, separate models to answer the same questions the hierarchical model can answer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_xuB7xN2J8YF"
   },
   "source": [
    "## Method Criticism\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Once you have a working model that you pretty confident in, it is time to criticize it (i.e. make sure the modelled values are valid). There are several ways to do this, including:\n",
    "\n",
    "* posterior predictive checks\n",
    "* mock-up data analysis\n",
    "\n",
    "Model comparison or criticism is important because a model is just an inference; the techniques that are subsequently discussed may be used to evaluate the model or to determine the best model (in the case where several different models have been constructed to answer the same problem)\n",
    "\n",
    "Two important concepts in statistics are *overfitting* and *underfitting*, both which result from using too many or too few parameters in the model. <a id='over'>*Overfitting*</a> can occur when there are too many parameters to model the problem, and the model is just learning to fit the exact data it is given. <a id='under'>*Underfitting*</a> represents the opposite case, where too few parameters are used and the model is just too simplistic to accurately model the problem. You can read more about overfitting and underfitting [here](https://en.wikipedia.org/wiki/Overfitting).\n",
    "\n",
    "Overfitting and underfitting are just two of the potential problems that can occur when building a model - which is why it is important to check the model's validity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E2tdXT2hKAFb"
   },
   "source": [
    "## Posterior predictive checks\n",
    "A <a id='ppc'>*posterior predictive check*</a> checks the validity of your model by using the posterior generated from the model to predict future data and then checking how well the newly generated data matches the observed data. This concept is easily demonstrated with an example.\n",
    "\n",
    "Let's try a problem using the daily average temperatures in Honolulu, with a mean of 85 degrees and a standard deviation of 5 degrees.\n",
    "\n",
    "First, let's generate the data and plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "colab_type": "code",
    "id": "SiTeQ_V1KFk7",
    "outputId": "ffdd2a2b-0215-4b8f-b0ed-9b6f034bd08f"
   },
   "outputs": [],
   "source": [
    "# posterior predictive check\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# temperature data, make fake dataset with mean = 85 degrees, sd = 5 degrees\n",
    "mean = 85\n",
    "sd = 5\n",
    "time = 365 * 30\n",
    "\n",
    "# generate temperature data\n",
    "data = np.random.normal(mean,sd,time)\n",
    "#random.seed()\n",
    "\n",
    "# plot of initial data\n",
    "sns.kdeplot(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zsfVPxOHKLQ6"
   },
   "source": [
    "We now have our observed data, let's proceed with constructing and running a model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 726
    },
    "colab_type": "code",
    "id": "BAAEBfxoKQCA",
    "outputId": "5a2064bd-6a4d-4bf2-88e4-77f1c8a27c14"
   },
   "outputs": [],
   "source": [
    "# model\n",
    "\n",
    "with pm.Model() as temp_model:\n",
    "    mu = pm.Uniform('mu', 60, 110)\n",
    "    s = pm.Normal('s', mu = 0, sd = 5)\n",
    "    sigma = pm.Deterministic('sigma', pm.math.abs_(s))\n",
    "    y = pm.Normal('y', mu=mu, sd=sigma, observed=data)\n",
    "    temp_norm = pm.sample(1100)\n",
    "  \n",
    "chain_norm = temp_norm[100:]\n",
    "pm.traceplot(chain_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yxvHqqntKVPC"
   },
   "source": [
    "We can also print out a summary of the model results using the summary function in PyMC3:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "colab_type": "code",
    "id": "SFiWGoXFKWQm",
    "outputId": "e77823ff-1b77-4e85-ed5d-d0dcc1c7552d"
   },
   "outputs": [],
   "source": [
    "# summary of model results\n",
    "\n",
    "pm.summary(chain_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7aUBOgc8Kihu"
   },
   "source": [
    "Now that we have our modelled results, we can perform a posterior predictive check using the `sample_ppc `\n",
    "function which is included in the PyMC3 package. After the posterior predictive check is performed, we can plot our data with our modelled predictions to compare results and see if the model we created is a good fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 471
    },
    "colab_type": "code",
    "id": "OWKEZDTmKjKf",
    "outputId": "0369b00d-345d-43c0-ad28-6af1ee18c127"
   },
   "outputs": [],
   "source": [
    "# perform posterior predictive check and plot comparison on results\n",
    "\n",
    "# generate a posterior predictive check by drawing 100 generated data sets \n",
    "# from modelled results using the sample_ppc function\n",
    "\n",
    "ppc = pm.sample_ppc(chain_norm, samples=100, model=temp)\n",
    "\n",
    "# plot ppc samples (grey) with observed data (black) to compare\n",
    "for i in ppc['y']:\n",
    "  sns.kdeplot(i, c='grey', alpha = 0.1)\n",
    "\n",
    "sns.kdeplot(data, c='k')\n",
    "plt.xlim(60, 100)\n",
    "plt.title('Posterior Predictive Check', fontsize=16)\n",
    "plt.xlabel('Temperature (degrees F)', fontsize=14)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QIlz2XxSK0GL"
   },
   "source": [
    "For this example, the posterior predictive check matches the observed data well, so we can be confident in our model. Of course, in this example we generated fake data from a normal distribution, so our model results match our observed data extremely well. For a real dataset with outliers, the match between the observed data and the modelled data will likely not be so similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QMovVTJDK-El"
   },
   "source": [
    "## Mock-up Data Analysis\n",
    "\n",
    "Another way to test our model is to perform a mock-up data analysis. In this case we don't even need the observed data, we are going to construct it ourselves. \n",
    "\n",
    "We need to choose the pre-set parameter and generate the data with this parameter using a generative model. After that we use the model one more time to infer back the parameter. If the model fails the task, then there is a chance that it is not right for the problem.\n",
    "\n",
    "We are going to use the example introduced above to perform the mock-up data analysis. The model we were using is the following:\n",
    "\n",
    "```\n",
    "with pm.Model() as temp_model:\n",
    "  mu = pm.Uniform('mu', 60, 110)\n",
    "  s = pm.Normal('s', mu = 0, sd = 5)\n",
    "  sigma = pm.Deterministic('sigma', pm.math.abs_(s))\n",
    "  y = pm.Normal('y', mu=mu, sd=sigma, observed=data)\n",
    "```\n",
    "There are two parameters: mu and sigma. Let's assign some values to them, suppose $mu=70$ and $sigma=5$. Then we can generate mock-up data the following way:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hvgq0gjxLJMV"
   },
   "outputs": [],
   "source": [
    "import pymc3 as pm\n",
    "\n",
    "mu = 70\n",
    "sigma = 5\n",
    "\n",
    "y_ = pm.Normal.dist(mu,sigma,shape = 1000)\n",
    "data = y_.random()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6DIC6GD2LPmT"
   },
   "source": [
    "Next step is to use the model to infer the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "-mdhaTD8LU--",
    "outputId": "0aacd422-4370-4e79-846d-08eab6f741a9"
   },
   "outputs": [],
   "source": [
    "with pm.Model() as temp_model:\n",
    "  mu = pm.Uniform('mu', 60, 110)\n",
    "  s = pm.Normal('s', mu = 0, sd = 5)\n",
    "  sigma = pm.Deterministic('sigma', pm.math.abs_(s))\n",
    "  y = pm.Normal('y', mu=mu, sd=sigma, observed=data)\n",
    "  temp_norm = pm.sample(1100)\n",
    "  \n",
    "chain_norm = temp_norm[100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "colab_type": "code",
    "id": "TZ-tUXx3LVpv",
    "outputId": "35ed5ede-2243-439b-b6b1-5f83628f4655"
   },
   "outputs": [],
   "source": [
    "pm.plot_posterior(chain_norm, varnames=['mu','sigma'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dSneH-oILm7t"
   },
   "source": [
    "As we can see, the results match chosen parameters well. Therefore, there is a good chance the model will succeed with real data.\n",
    "\n",
    "These are just two ways of checking a model's validity, but there are many more. If you are interested in reading more, some good resources can be found [here](https://am207.github.io/2018spring/wiki/bioassay_pymc3.html#model-checking) and [here](https://docs.pymc.io/api/diagnostics.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hrwqyMRkEv-v"
   },
   "source": [
    "## Mixture models\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "A <a id='mm'>*mixture model*</a> is useful when the problem you are trying to solve is comprised of several distributions (i.e. a mixture of distributions). For instance, let's say you are purchasing produce at the grocery store and want to compare the price of a banana picked up at random. The grocery store has two types of bananas for sale - organic (typically more expensive) and non-organic (typically cheaper) and can be modelled with a Gaussian distribution. When a mixture model uses a Gaussian distribution, it is called a <a id='gmm'>*Gaussian Mixture Model*</a> (abbreviated GMM). In this case, the data will show several distributions, as shown below in the figure:\n",
    "\n",
    "%%insert graphic like this, with grocery store example (i.e. replace 'waiting' with price per banana, and the probabilities to match below) below illustrated: \n",
    "\n",
    "![alt text](media/chapter4/mixture.png)\n",
    "\n",
    "In the above figure, the red and blue distributions represent different <a id='mc'>*mixture components*</a>, or distributions with different parameters (such as the mean and variance for a Gaussian distribution). For the bananas, the two mixture components would be non-organic (in red on the plot, where mean = $80$ cents per banana and standard deviation = $8$ cents) and organic (in blue on the plot, where mean = $95$ cents per banana and standard deviation = $12$ cents). A <a id='mp'>*mixture proportion*</a> describes the probability of choosing a non-organic or organic banana at random. In this case, grocery store stocks $65%$ non-organic bananas and $35%$ organic bananas.\n",
    "\n",
    "\n",
    "**To recap, our mixture components (the number of mixture components is written as $k$) are** (where subscript $n$ refers to not organic and $o$ refers to organic):\n",
    "\n",
    "* non-organic ($\\mu_n$ = $80$ cents, $\\sigma^2_n$ = $8$ cents)\n",
    "* organic ($\\mu_o$ = $95$ cents, $\\sigma^2_o$ = $12$ cents)\n",
    "\n",
    "**Our mixture proportions (written as $\\pi_k$) are:**\n",
    "* non-organic ($\\pi_n$ = 0.65)\n",
    "* organic ($\\pi_o$ = 0.35)\n",
    "\n",
    "A *mixture model*, in other words, describes data which originate from one of $k$ mixture components.\n",
    "\n",
    "Let's try an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fd4nddWdSfkG"
   },
   "source": [
    "### Example 4.2 \n",
    "Suppose we go to different stores in Honolulu and buy 100 bananas. Some of them are organic and some of them are not. We only keep track of the price of each banana. What is a probability that a random banana out of this 100 will be organic?\n",
    "\n",
    "Using the mixture components above, we want to create a generative model that would describe this problem. Let $p$ be the mixture proportion of organic bananas ($\\pi_o=p$). Then mixture proportion of non-organic bananas is $\\pi_n=1-p$. Since we don't know anything about the proportions, we can assume $p$ is chosen uniformly between $0$ and $1$.The price of each banana can be determined by using Gaussian distributions with parameters specified above (i.e., $\\mu_n=.80, \\sigma_n^2=0.08, \\mu_o=0.95,\\sigma_o^2=0.12$). The last step is to \"mix\" all of the above information into one model. This can be done using the pm.NormalMixture function.\n",
    "\n",
    "The model can be implemented in PyMC3 the following way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 634
    },
    "colab_type": "code",
    "id": "q43Ah1iEh-ta",
    "outputId": "857832e1-6be8-4ecb-b443-d36a88c74bb5"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "# generating the data used for the model\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(4500)\n",
    "# define the standard deviation (sigma) and mean\n",
    "mu_n = 0.80\n",
    "mu_o = 0.95\n",
    "sd_n = 0.08\n",
    "sd_o = 0.12\n",
    "\n",
    "N_o = 20\n",
    "N_n = 80\n",
    "\n",
    "x_1 = np.random.normal(mu_o, sd_o, N_o)\n",
    "x_2 = np.random.normal(mu_n, sd_n, N_n)\n",
    "\n",
    "x = np.append(x_1, x_2)\n",
    "plt.hist(x,10)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "id": "qT1LOEvPZBhc",
    "outputId": "f485ea67-e3c8-48fe-8710-5ba7e33b6170"
   },
   "outputs": [],
   "source": [
    "import pymc3 as pm\n",
    "import numpy as np\n",
    "\n",
    "data = x\n",
    "\n",
    "mu_n = 0.80\n",
    "mu_o = 0.95\n",
    "sd_n = 0.08\n",
    "sd_o = 0.12\n",
    "\n",
    "with pm.Model() as banana_model:\n",
    "    p = pm.Uniform('p', 0, 1)\n",
    "    mixture = pm.NormalMixture('mix', w=[p, 1 - p], mu=[mu_o, mu_n], \n",
    "                               sd=np.array([sd_o,sd_n]), observed = data)\n",
    "    trace = pm.sample(20000)\n",
    "    burned_trace = trace[1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 418
    },
    "colab_type": "code",
    "id": "OUlJrryVb8gG",
    "outputId": "fb0e3989-1312-4efe-d101-9b529e4dccb2"
   },
   "outputs": [],
   "source": [
    "pm.plot_posterior(burned_trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j7qVrcRps5-k"
   },
   "source": [
    "This simple mixture model resulted in about a 30% mean probability (with a 95% HPD from about 18 to 45%) of randomly picking an organic banana out of the 100 we purchased. This makes sense because you are less likely to pick up an organic banana compared to a non-organic banana.\n",
    "\n",
    "Let's try a mixture model that's a little more complicated now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "syKoD8xq1CzX"
   },
   "source": [
    "### Example 4.3\n",
    "\n",
    "There are three types of fish living in a pond. [Researchers](https://www.researchgate.net/lab/Yoshizawa-Lab-Masato-Yoshizawa) have determined that the only distinguishing features between them are their light preferences:\n",
    "\n",
    "\n",
    "1.   Fish that likes to be in the light. Usually spends around $0.87$ of the time in the light.\n",
    "2.   Fish that likes to be in the dark. It spends around $0.18$ of the time in the light.\n",
    "3.   Fish that likes to be in between light and dark. It spends around $0.51$ of the time in the light.\n",
    "\n",
    "It was also determined that standard deviation is the same for all three types of fish and is $0.11$.** Therefore, our mixture components ($k$) are:**\n",
    "\n",
    "* Type 1 Fish - preference for light ($\\mu$  =  0.87, $\\sigma^2$  =  0.11)\n",
    "* Type 2 Fish - preference for dark ($\\mu$  =  0.18,  $\\sigma^2$  =  0.11)\n",
    "* Type 3 Fish - preference for both light and dark ($\\mu$  =  0.51,  $\\sigma^2$ =  0.11)\n",
    "\n",
    "\n",
    "We observe the pond for one hour and record how much time each fish stays in the light. We want to determine how many fish of each type are in the pond.\n",
    "\n",
    "---\n",
    "This problem can be solved using a mixture model.\n",
    "\n",
    "The first step, as usual, is to create a generative model that describes the problem. \n",
    "The observed data is a vector where i-th element is a value between $0$ and $1$ that corresponds to the time i-th fish spent in the light. So, we are given the number of fish in the pond, $N$, that is the length of the vector.\n",
    "\n",
    "The model consists of two important parts: the proportions of different types in a pond (weights) and the distributions that are used to determine the time spent in the light for each type of fish.\n",
    "\n",
    "The distribution that we are going to use to generate weights is called the <a id='dirichlet'>*Dirichlet distribution*</a> with parameters $a = (a_1,a_2,...,a_N)$. This distribution divides the interval $(0,1)$ into $N$ pieces of lengths $w_1,w_2,...,w_N$; all that will add up to one. That is exactly what we need for determining our weights (or mixture proportions, $\\pi$). We also will choose an uninformative prior $a=(1,1,...,1)$\n",
    "\n",
    "Sidenote: The Dirichlet distribution with $N=2$ is a Beta distribution.\n",
    "\n",
    "Because we know the mean and standard deviation for each type of fish, we are going to use the Gaussian distribution to determine how much time fish spends in the light:\n",
    "\n",
    "1.   Gaussian(0.87, 0.11)\n",
    "2.   Gaussian(0.18, 0.11)\n",
    "3.   Gaussian(0.51, 0.11)\n",
    "\n",
    "Now, the last step of the generative model is to \"mix\" all of the above information into one model. This can be done in PyMC3 using the `pm.Mixture` or `pm.NormalMixture` function. Since our distributions are Gaussian (normal), we are going to use the latter one.\n",
    "\n",
    "This model can be implemented in PyMC3 the following way:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "eeSicvhAOnsd"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "# generating the data used for the model\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(4500)\n",
    "# define the standard deviation (sigma) and mean\n",
    "sigma = 0.11\n",
    "mean_1 = 0.87\n",
    "mean_2 = 0.18\n",
    "mean_3 = 0.51\n",
    "N1 = 500\n",
    "N2 = 700\n",
    "N3 = 800\n",
    "\n",
    "x_1 = np.random.normal(mean_1, sigma, N1)\n",
    "x_2 = np.random.normal(mean_2, sigma, N2)\n",
    "x_3 = np.random.normal(mean_3, sigma, N3)\n",
    "\n",
    "x_1 = (x_1 > 1) * np.random.uniform(mean_1, 1, N1) + (x_1 <= 1) * x_1\n",
    "x_2 = (x_2 < 0) * np.random.uniform(0, mean_2 + 0.05, N2) + (x_2 >= 0) * x_2\n",
    "\n",
    "x = np.append(x_1, x_2)\n",
    "x = np.append(x, x_3)\n",
    "\n",
    "#nbins = 30\n",
    "\n",
    "#plt.hist(x,nbins)\n",
    "#print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "id": "BmXpA8yVRODe",
    "outputId": "d5f4b3a9-aba6-474c-97d0-423480995296"
   },
   "outputs": [],
   "source": [
    "import pymc3 as pm\n",
    "import numpy as np\n",
    "\n",
    "data = x\n",
    "N = len(data)\n",
    "\n",
    "mu_1 = 0.87\n",
    "mu_2 = 0.18\n",
    "mu_3 = 0.51\n",
    "sd = 0.11\n",
    "\n",
    "with pm.Model() as fish_model:\n",
    "    weights = pm.Dirichlet('weights', a=np.array([1.0, 1.0, 1.0]))\n",
    "    mixture = pm.NormalMixture('mix', w=weights, mu=[mu_1,mu_2,mu_3], \n",
    "                               sd=sd, observed=data)\n",
    "    #step = pm.Metropolis()\n",
    "    trace = pm.sample(20000)\n",
    "    burned_trace = trace[1000:]\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 432
    },
    "colab_type": "code",
    "id": "eigb1Fh2Jobd",
    "outputId": "19ebbdad-61bb-4063-f333-3664f6c698d0"
   },
   "outputs": [],
   "source": [
    "pm.plot_posterior(burned_trace, varnames=['weights'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qkhwp8c6Dvtp"
   },
   "source": [
    "Note that it is not necessary for us to know the means for the Gaussian distributions we used above. The model can find the means that fit the observed data. So, let's try to set $\\mu_1, \\mu_2, \\mu_3$ as priors drawn from Uniform distributions. We can choose these priors to be informative based on the light preferences of each group of fish. So, let mu_1~Uniform(0.75,1), mu_2~Uniform(0,0.25), mu_3~Uniform(0.4,0.6). The implemented model looks the following way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5uzVuLwfFAJK"
   },
   "outputs": [],
   "source": [
    "import pymc3 as pm\n",
    "import numpy as np\n",
    "\n",
    "data = x\n",
    "N = len(data)\n",
    "\n",
    "sd = 0.11\n",
    "\n",
    "with pm.Model() as fish_model:\n",
    "    mu_1 = pm.Uniform('mu_1', 0.75, 1)\n",
    "    mu_2 = pm.Uniform('mu_2', 0, 0.25)\n",
    "    mu_3 = pm.Uniform('mu_3', 0.4, 0.6)\n",
    "    weights = pm.Dirichlet('weights', a=np.array([1.0, 1.0, 1.0]))\n",
    "    mixture = pm.NormalMixture('mix', w=weights, mu=[mu_1,mu_2,mu_3], \n",
    "                               sd=sd, observed=data)\n",
    "    #step = pm.Metropolis()\n",
    "    trace = pm.sample(20000)\n",
    "    burned_trace = trace[1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IRk4z-v9FKQh"
   },
   "outputs": [],
   "source": [
    "pm.plot_posterior(burned_trace, varnames=['weights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9s-0gTsnFM-M"
   },
   "outputs": [],
   "source": [
    "pm.plot_posterior(burned_trace, varnames=['mu_1','mu_2','mu_3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z1t8JGUUFToW"
   },
   "source": [
    "As you can see, the posterior distributions of weights are about the same as in the model with fixed means in Gaussian distributions. Moreover, posterior distributions of means are also acuurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l1lQwLck13FJ"
   },
   "source": [
    "The mixture model allowed us to partition the proportion of each type of fish. In this mixture model, we did not apply our assumptions about the means beforehand in the model, instead we used an uniformative prior. The results of the mixture model show that fish type 1 (the one that likes light) makes up about 26% (with a 95% HPD of about 24 to 28%) of the fish in the pond, fish type 2 (the one that likes the dark) makes up about 36% (with a 95% HPD of about 33 to 38%) of the fish in the pond, and fish type 3 (the one that likes both light and dark) is makes up about 38% (with a 95% HPD of about 36 to 41%) in the pond.\n",
    "\n",
    "Go ahead and try to do the same problem, this time using an informative prior (i.e. using the means for each type of fish in the model instead of the uniformative prior). The resulting posterior distributions should be very similar.\n",
    "\n",
    "In this chapter we discussed two different Gaussian mixture models, however other distributions can also be used (for instance, the Poisson distribution is commonly used for mixture models dealing with count data). Mixture models are useful for clustering multiple distributions within a population, such as the probability of picking an organic banana at random or partitioning the proportion of types of fish in a pond."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgTXaSQiPvDC"
   },
   "source": [
    "---\n",
    "\n",
    "This chapter concludes the Introduction to Probabilistic Programming module. In this module, we learned the how to construct a generative model, basics of probabilistic programming with the PyMC3 package in Python, several common probability distributions and their applications, Bayesian statistics, ways to model a problem, and how to check a model. With this introductory course, you should be able to construct simple models and have the framework to approach more complicated ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RaQEtjyZu75J"
   },
   "source": [
    "### <u>Definitions</u>\n",
    "\n",
    "<a href='#mm'>**mixture model**</a>: a type of model that is used to differentiate groups within a population by parsing different distributions.\n",
    "\n",
    "<a href='#gmm'>**Gaussian mixture model (GMM)**</a>: a mixture model that uses the Gaussian distribution.\n",
    "\n",
    "<a href='#mc'>**mixture component ($k$)**</a>: part of a mixture model, refers to distributions with different parameters.\n",
    "\n",
    "<a href='#mp'>**mixture proportion ($\\pi$)**</a>: part of a mixture model, refers to the proportion or weights of each group.\n",
    "\n",
    "<a href='#dirichlet'>**Dirichlet distribution**</a>: a probability distribution where each number in the set must add up to 1, commonly used when working with categorical distributions.\n",
    "\n",
    "<a href='#hier'>**hierarchical (multi-level) model**</a>: a type of probabilistic model that has layered dependencies.\n",
    "\n",
    "<a href='#latent'>**latent variables**</a>: variables that can be inferred, but not directly observed.\n",
    "\n",
    "<a href='#hyperprior'>**hyperprior**</a>: prior of a hyperparameter\n",
    "\n",
    "<a href='#ppc'>**posterior predictive check**</a>: a way to check model validity using the posterior to predict future data.\n",
    "\n",
    "<a href='#hpd'>**Highest Posterior Density (HPD)**</a>: the range of values for which the posterior distribution are most likely to occur. This is often given at the 95% confidence interval.\n",
    "\n",
    "<a href='#over'>**overfitting**</a>: when too many parameters are used in a model given the data provided, resulting in a model that is too complex and not an accurate representation of the problem.\n",
    "\n",
    "<a href='#under'>**underfitting**</a>: when too few parameters are used in a model given the data provided, resulting in a model that is too simple and not an accurate representation of the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "PP_4_Examples.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
